{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Post-processing of numerical datset**\n",
    "\n",
    "<u>To do :</u>\n",
    "\n",
    "The processed dataset is in a numerical format. \n",
    "\n",
    "However, it still requires post-processing to ensure good machine learning performance.\n",
    "\n",
    "The post-processing is a 2-steps process :\n",
    "- Normalization\n",
    "- Dimension Reduction\n",
    " \n",
    "<u>Note :</u> Replace whenever you find #? by a required python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load the processed numerical dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salaire</th>\n",
       "      <th>prime</th>\n",
       "      <th>anciennete</th>\n",
       "      <th>etat civil celibataire</th>\n",
       "      <th>etat civil marie</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nom</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ali</th>\n",
       "      <td>1200.675000</td>\n",
       "      <td>100.560</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sonia</th>\n",
       "      <td>2800.786000</td>\n",
       "      <td>400.876</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rahma</th>\n",
       "      <td>2192.082091</td>\n",
       "      <td>130.987</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           salaire    prime  anciennete  etat civil celibataire  \\\n",
       "nom                                                               \n",
       "Ali    1200.675000  100.560           5                     1.0   \n",
       "Sonia  2800.786000  400.876          18                     0.0   \n",
       "Rahma  2192.082091  130.987           6                     1.0   \n",
       "\n",
       "       etat civil marie  \n",
       "nom                      \n",
       "Ali                 0.0  \n",
       "Sonia               1.0  \n",
       "Rahma               0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load processed numerical employees as a dataframe\n",
    "df_employees = pd.read_csv('employes_num.csv', header=0, index_col=0)\n",
    "df_employees.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the dataset becomes a numerical matrix, we have to import numpy module\n",
    "# The subsequent processing will be performed on a matrix\n",
    "import numpy as np\n",
    "\n",
    "# To limit the precision of float numbers to 2 when displaying numpy arrays\n",
    "# Use this trick :\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Normalization of the columns**\n",
    "\n",
    "Column scales are not balanced\n",
    "\n",
    "They require a normalization processing\n",
    "\n",
    "We can check this scale unbalancing in means and variances of variables 'salaire' , 'prime' , 'anciennete' , ... or by the covariance matrix of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.20e+03, 1.01e+02, 5.00e+00, 1.00e+00, 0.00e+00],\n",
       "       [2.80e+03, 4.01e+02, 1.80e+01, 0.00e+00, 1.00e+00],\n",
       "       [2.19e+03, 1.31e+02, 6.00e+00, 1.00e+00, 0.00e+00],\n",
       "       [2.50e+03, 3.41e+02, 1.30e+01, 0.00e+00, 1.00e+00],\n",
       "       [3.10e+03, 2.57e+02, 1.90e+01, 0.00e+00, 1.00e+00],\n",
       "       [1.30e+03, 1.51e+02, 6.00e+00, 1.00e+00, 0.00e+00],\n",
       "       [1.10e+03, 1.31e+02, 4.00e+00, 1.00e+00, 0.00e+00],\n",
       "       [3.00e+03, 2.57e+02, 2.30e+01, 0.00e+00, 1.00e+00],\n",
       "       [1.51e+03, 1.60e+02, 6.00e+00, 1.00e+00, 0.00e+00],\n",
       "       [2.70e+03, 4.00e+02, 2.40e+01, 0.00e+00, 1.00e+00],\n",
       "       [1.20e+03, 2.57e+02, 8.00e+00, 1.00e+00, 0.00e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the numerical values of the dataset df_ as a numpy matrix X\n",
    "X=df_employees.values\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check data statistics**\n",
    "\n",
    "To justify the normalization, we should be sure that the data statistics are unbalanced mainly means and variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.05e+03, 2.35e+02, 1.20e+01, 5.45e-01, 4.55e-01])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the means of the dataset X\n",
    "# To compute the mean per column of a matrix\n",
    "# Call np.mean() and pass as arguments :\n",
    "# - the matrix\n",
    "# - axis=0\n",
    "np.mean(X , axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.84e+05, 1.09e+04, 5.35e+01, 2.48e-01, 2.48e-01])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the variances of the dataset X\n",
    "# To compute the mean per column of a matrix\n",
    "# Call np.var() and pass as arguments :\n",
    "# - the matrix\n",
    "# - axis=0\n",
    "np.var(X , axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Standard Scaling process**\n",
    "\n",
    "Standard Scaling is implemented using `sklearn.preprocessing.StandardScaler`\n",
    "\n",
    "It is a way to normalize the data so that it has : \n",
    "- a mean (average) of 0\n",
    "- a standard deviation (variance square) of 1. \n",
    "\n",
    "Process description :\n",
    "\n",
    "1. Calculate Mean and Standard Deviation per columns\n",
    "\n",
    "2. Subtract Mean: It then subtracts the mean from each data point. \n",
    "   \n",
    "   -> This operation centers the data around 0, so the new mean is 0.\n",
    "\n",
    "3. Divide by Standard Deviation: After centering, each data point is divided by the standard deviation. \n",
    "\n",
    "   -> This step scales the data, making the standard deviation equal to 1.\n",
    "\n",
    "   Once the normalization is applied, all the data variables (dataset columns) have 0 (means) and 1 (variance) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler class from sklearn.preprocessing module\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of StandardScaler class\n",
    "ss=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call fit() function of ss and pass X as argument\n",
    "ss.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.05e+03, 2.35e+02, 1.20e+01, 5.45e-01, 4.55e-01])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.84e+05, 1.09e+04, 5.35e+01, 2.48e-01, 2.48e-01])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.12, -1.29, -0.96,  0.91, -0.91],\n",
       "       [ 0.98,  1.59,  0.82, -1.1 ,  1.1 ],\n",
       "       [ 0.18, -1.  , -0.82,  0.91, -0.91],\n",
       "       [ 0.58,  1.01,  0.14, -1.1 ,  1.1 ],\n",
       "       [ 1.37,  0.21,  0.96, -1.1 ,  1.1 ],\n",
       "       [-0.99, -0.81, -0.82,  0.91, -0.91],\n",
       "       [-1.25, -1.  , -1.09,  0.91, -0.91],\n",
       "       [ 1.24,  0.21,  1.5 , -1.1 ,  1.1 ],\n",
       "       [-0.72, -0.72, -0.82,  0.91, -0.91],\n",
       "       [ 0.84,  1.58,  1.64, -1.1 ,  1.1 ],\n",
       "       [-1.12,  0.21, -0.55,  0.91, -0.91]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call transform() of ss and pass X as argument\n",
    "# It returns the normalized dataset, denoted X_ss\n",
    "X_ss = ss.transform(X)\n",
    "X_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check statistics of normalized dataset**\n",
    "\n",
    "After the normalization, we can check the data statistics are now balanced mainly means and variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.21e-16, -1.26e-17,  0.00e+00,  1.21e-16, -6.06e-17])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After noramlization, check the means of the columns of dataset X\n",
    "# They should be almost 0\n",
    "np.mean(X_ss , axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After normalization, check the variances or covariance matrix of dataset\n",
    "# The variance values should be 1\n",
    "np.var(X_ss , axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Reduction of the dimension**\n",
    "\n",
    "The numerical data matrix serves as a representation of population data.\n",
    "\n",
    "The rows within this matrix represent individuals within the population.\n",
    "\n",
    "The columns of this matrix correspond to characteristics that describe the population.\n",
    "\n",
    "Because the data matrix is normalized, it becomes possible to compare these characteristics, revealing potential redundancies.\n",
    "\n",
    "In such cases, reducing the number of dimensions helps to eliminate redundant information.\n",
    "\n",
    "We can assess the <font color='red'>correlation</font> between these dimensions by examining the covariance matrix of the normalized data matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check dataset statistics**\n",
    "\n",
    "To justify the dimension reduction, we should be sure that there are <font color='red'>high correlations</font> between dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  0.69,  0.89, -0.92,  0.92],\n",
       "       [ 0.69,  1.  ,  0.8 , -0.84,  0.84],\n",
       "       [ 0.89,  0.8 ,  1.  , -0.92,  0.92],\n",
       "       [-0.92, -0.84, -0.92,  1.  , -1.  ],\n",
       "       [ 0.92,  0.84,  0.92, -1.  ,  1.  ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the covarinace matrix of normalized dataset\n",
    "# Mainly the non-diagonal values that correpond to correlations between the variables (salaire, prime, ...)\n",
    "# They are values between [-1, 1]\n",
    "# If they are close to 0 , it means that there is no correlation\n",
    "# If they are close to 1 , it means that there is a high correlation\n",
    "# In this case, it is worth to reduce the dimension (number of variables) \n",
    "np.cov(X_ss.T , ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA class from sklearn.decomposition module\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of PCA class\n",
    "pca=PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PCA()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the function fit() and pass X_ss as an argument\n",
    "pca.fit(X_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.01e-01, 6.41e-02, 2.05e-02, 1.45e-02, 1.48e-33])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the weight of each variable in \n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.50e+00,  2.08e-15, -2.98e-16, -6.56e-17,  6.52e-16],\n",
       "       [ 2.08e-15,  3.21e-01, -7.63e-18, -1.68e-17, -3.04e-17],\n",
       "       [-2.98e-16, -7.63e-18,  1.02e-01, -3.69e-17,  1.36e-17],\n",
       "       [-6.56e-17, -1.68e-17, -3.69e-17,  7.26e-02,  3.68e-17],\n",
       "       [ 6.52e-16, -3.04e-17,  1.36e-17,  3.68e-17,  1.19e-31]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_pca=pca.transform(X_ss)\n",
    "# np.cov(X_pca.T , ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dimension reduction using PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.3 , -0.25],\n",
       "       [ 2.47,  0.57],\n",
       "       [-1.55, -0.73],\n",
       "       [ 1.76,  0.37],\n",
       "       [ 2.14, -0.8 ],\n",
       "       [-1.99,  0.07],\n",
       "       [-2.3 ,  0.08],\n",
       "       [ 2.33, -0.79],\n",
       "       [-1.83, -0.01],\n",
       "       [ 2.78,  0.55],\n",
       "       [-1.5 ,  0.95]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Apply PCA with reduced dimension=2\n",
    "pca=PCA(n_components=2)\n",
    "pca.fit(X_ss)\n",
    "X_pca=pca.transform(X_ss)\n",
    "X_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check statistics of reduced dataset**\n",
    "\n",
    "Following dimension reduction, we examine the covariance matrix of the reduced data matrix to confirm that there no longer correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.50e+00, 2.08e-15],\n",
       "       [2.08e-15, 3.21e-01]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the covarinace matrix of reduced dataset\n",
    "# Check that the non-diagonal values are reduced to 0.\n",
    "np.cov(X_pca.T , ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the transformed dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the reduced normalized dataset, call to_csv() function and pass as arguments :\n",
    "# - file name 'employes_num_2.csv'\n",
    "# - header : None because the columns have no labels\n",
    "# - index : True in order to put employees names as index\n",
    "df_employes_proc=pd.DataFrame(X_pca, index=df_employees.index)\n",
    "df_employes_proc.to_csv('employes_num_2.csv', header=None, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
